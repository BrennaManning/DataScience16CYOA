{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SF-Crime Kaggle Challenge      2.23.16\n",
    "### Data Science 2016 - CYOA Project.\n",
    "_______________________________________________________\n",
    "\n",
    "### Olin College\n",
    "### David Abrahams & Brenna Manning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We learned some cool things and we would like to share our work with you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from patsy import dmatrix\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_dir = os.path.dirname('__file__')\n",
    "\n",
    "train = pd.read_csv(os.path.join(cur_dir, \"data\", \"train.csv\"))\n",
    "test = pd.read_csv(os.path.join(cur_dir, \"data\", \"test.csv\"))\n",
    "mapdata = np.loadtxt(os.path.join(cur_dir, \"data\", \"sf_map.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helper functions. Note we do all our preprocessing in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_subset(df, n=None):\n",
    "    if (n is None):\n",
    "        return df\n",
    "    sub = random.sample(xrange(len(df)), min(n, len(df)))\n",
    "    return df.iloc[sub]\n",
    "\n",
    "def preprocess(df, drop=True):\n",
    "    res = df.copy()\n",
    "    if (drop):\n",
    "        res = res[res.X != res.X.max()]\n",
    "    datetimes = res.Dates.apply(get_datetime)\n",
    "    res['Hour'] = datetimes.apply(lambda dt: dt.hour)\n",
    "    res['Hour_Reordered'] = datetimes.apply(lambda dt: (dt.hour - 5) % 24)\n",
    "    res['Month'] = datetimes.apply(lambda dt: dt.month)\n",
    "    res['Hour_Minutes'] = datetimes.apply(lambda dt: dt.hour + dt.minute / 60.0)\n",
    "    res['Minute'] = datetimes.apply(lambda dt: dt.minute)\n",
    "    res['Year'] = datetimes.apply(lambda dt: dt.year)\n",
    "    res['Minutes_Since_03'] = datetimes.apply(lambda dt: (dt-datetime(2003, 1, 1)).total_seconds() / 60)\n",
    "    res['Minutes_Since_New_Year'] = datetimes.apply(lambda dt: (dt-datetime(dt.year, 1, 1)).total_seconds() / 60)\n",
    "    res['Street_Corner'] = res['Address'].apply(lambda x: 1 if '/' in x else 0)\n",
    "    res['Weekend'] = res.DayOfWeek.apply(lambda x: \"Weekend\" if x in [\"Saturday\", \"Sunday\"] else \"Weekday\")\n",
    "    res['WorkNextDay'] = res.DayOfWeek.apply(lambda x: \"No Work\" if x in [\"Saturday\", \"Friday\"] else \"Work\")\n",
    "    res['DOW_Index'] = res.DayOfWeek.apply(lambda x: dow.index(x))\n",
    "    res['DayorNight'] = res.Hour_Minutes.apply(lambda x: \"Day\" if x > 6 and x < 20 else \"Night\")\n",
    "    res['Midnight'] = (res.Hour == 0) & (res.Minute == 1)\n",
    "    res['Noon'] = (res.Hour == 12) & (res.Minute == 0)\n",
    "    \n",
    "    return res\n",
    "\n",
    "dow = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "def get_datetime(s):\n",
    "    dt = datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = preprocess(get_random_subset(train))\n",
    "all_categories = train_df.Category.value_counts().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for data exploration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"Category\", data=train_df)\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=90)\n",
    "plt.gcf().set_size_inches(16, 6, forward=True)\n",
    "plt.title(\"Most common crimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_cats = all_categories.drop(\"OTHER OFFENSES\")[0:5]\n",
    "most_common_districs = list(train_df.PdDistrict.value_counts().index[0:4])\n",
    "most_common_districs.append('TENDERLOIN') # Because Tenderloin has all the drugs.\n",
    "\n",
    "freqs = pd.DataFrame({'count' : train_df.groupby( [\"Category\", \"PdDistrict\"] ).size()}).reset_index()\n",
    "wknd_totals = freqs.PdDistrict.map(dict(train_df.groupby( \"PdDistrict\" ).size()))\n",
    "freqs[\"freq\"] = freqs[\"count\"].divide(wknd_totals)\n",
    "freqs_filt = freqs[freqs.Category.isin(most_common_cats) & freqs.PdDistrict.isin(most_common_districs)]\n",
    "\n",
    "sns.barplot(data=freqs_filt, x=\"PdDistrict\", y=\"freq\", hue=\"Category\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.gcf().set_size_inches(16, 6, forward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This graph shows the frequency for each crime in each district. Interesting that drugs make up rougly 6% of crimes in most districts, but 22% in Tenderloin. Also pretty cool how much more common Larceny is in Northern and Southern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out if weekends are special:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_cats = all_categories[0:8]\n",
    "\n",
    "freqs = pd.DataFrame({'count' : train_df.groupby( [\"Category\", \"Weekend\"] ).size()}).reset_index()\n",
    "wknd_totals = freqs.Weekend.map(dict(train_df.groupby( \"Weekend\" ).size()))\n",
    "freqs[\"freq\"] = freqs[\"count\"].divide(wknd_totals)\n",
    "freqs_filt = freqs[freqs.Category.isin(most_common_cats)]\n",
    "\n",
    "sns.barplot(data=freqs_filt, x=\"Category\", y=\"freq\", hue=\"Weekend\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.gcf().set_size_inches(16, 6, forward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well that's disappointing. Maybe there's something special about Fridays and Saturdays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs = pd.DataFrame({'count' : train_df.groupby( [\"Category\", \"WorkNextDay\"] ).size()}).reset_index()\n",
    "wknd_totals = freqs.WorkNextDay.map(dict(train_df.groupby( \"WorkNextDay\" ).size()))\n",
    "freqs[\"freq\"] = freqs[\"count\"].divide(wknd_totals)\n",
    "freqs_filt = freqs[freqs.Category.isin(most_common_cats)]\n",
    "\n",
    "sns.barplot(data=freqs_filt, x=\"Category\", y=\"freq\", hue=\"WorkNextDay\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.gcf().set_size_inches(16, 6, forward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nope again...Let's try plotting the frequency of each crime over the course of the days of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in all_categories[0:8]:\n",
    "    subset = train_df[train_df.Category == c]\n",
    "    groups = subset.groupby(\"DOW_Index\")\n",
    "    days = [g.DOW_Index.mean() for _, g in groups]\n",
    "    crimes = [len(g) for _, g in groups]\n",
    "    plt.plot(days, crimes, label=c)\n",
    "plt.legend(loc=2)\n",
    "plt.gcf().set_size_inches(16, 10, forward=True)\n",
    "plt.gca().set_xticklabels(dow);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems like Day of the Week on its own is not a good indicator for the most common crimes. Let's see if whether or not a crime occurred on a street corner (the address name contain a '/') was at all predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_cats = all_categories[0:8]\n",
    "\n",
    "freqs = pd.DataFrame({'count' : train_df.groupby( [\"Category\", \"Street_Corner\"] ).size()}).reset_index()\n",
    "wknd_totals = freqs.Street_Corner.map(dict(train_df.groupby( \"Street_Corner\" ).size()))\n",
    "freqs[\"freq\"] = freqs[\"count\"].divide(wknd_totals)\n",
    "freqs_filt = freqs[freqs.Category.isin(most_common_cats)]\n",
    "\n",
    "sns.barplot(data=freqs_filt, x=\"Category\", y=\"freq\", hue=\"Street_Corner\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.gcf().set_size_inches(16, 6, forward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is interesting. Assults don't happen on street corners, other offenses do. Let's see more granularity in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's investigate time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 24, 20, False)\n",
    "for c in all_categories[0:6]:\n",
    "    subset = train_df[train_df.Category == c]\n",
    "    indices = np.digitize(subset.Hour_Minutes, bins)\n",
    "    groups = subset.groupby(indices)\n",
    "    times = [g.Hour_Minutes.mean() for i, g in groups]\n",
    "    crimes = [len(g) for i, g in groups]\n",
    "    plt.plot(times, crimes, label=c)\n",
    "plt.legend()\n",
    "plt.gcf().set_size_inches(16, 6, forward=True)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Time of day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems like time of day is specifically important for Larceny/Theft. Let's break it down by day of the week for Larceny:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 24, 20, False)\n",
    "for d in dow:\n",
    "    subset = train_df[train_df.DayOfWeek == d]\n",
    "    indices = np.digitize(subset.Hour_Minutes, bins)\n",
    "    groups = subset.groupby(indices)\n",
    "    times = [g.Hour_Minutes.mean() for i, g in groups]\n",
    "    crimes = [len(g) for i, g in groups]\n",
    "    plt.plot(times, crimes, label=d)\n",
    "plt.legend()\n",
    "plt.gcf().set_size_inches(16, 6, forward=True)\n",
    "plt.ylabel(\"Larceny Frequency\")\n",
    "plt.xlabel(\"Time of day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's kinda interesting from roughly 8pm-6am Larceny is a lot more common on weekends, and from 6am-12pm it's more common on weekdays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's explore the location variables in our data set. First, we create some helper functions for making a hexbin graph, and then for comparing day vs. night and weekend vs. weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hexbin_image(df, title=None, color=plt.cm.Blues, vmax=250, cat=None):\n",
    "    \n",
    "    if (cat is not None):\n",
    "        df = df[df.Category == cat]\n",
    "    \n",
    "    lon_lat_box = (-122.5247, -122.3366, 37.699, 37.8299)\n",
    "\n",
    "    imgplot = plt.imshow(mapdata, cmap=plt.get_cmap('gray'),extent=lon_lat_box)\n",
    "    plt.hexbin(df.X, df.Y, gridsize=40, cmap=color,\n",
    "               extent=lon_lat_box, alpha=0.5, vmin=0, vmax=vmax)\n",
    "    if (title is None):\n",
    "        print 'hello'\n",
    "        plt.title('%s by Location' % df.Category.mode().values[0])\n",
    "    else:\n",
    "        plt.title(title)\n",
    "        \n",
    "def day_vs_night_plot(df, cat=None):\n",
    "    if (cat is not None):\n",
    "        df = df[df.Category == cat]\n",
    "    f, axtuple = plt.subplots(1, 2, sharey=True)\n",
    "    colors = {\"Day\": plt.cm.Oranges, \"Night\": plt.cm.Blues}\n",
    "    for i, light in enumerate([\"Day\", \"Night\"]):\n",
    "        subset = df[df.DayorNight == light]\n",
    "        plt.sca(axtuple[i])\n",
    "        hexbin_image(subset, title='%s during the %s' % (df.Category.mode().values[0], light), color=colors[light])        \n",
    "\n",
    "def weekday_weekend_plot(df, cat=None):\n",
    "    if (cat is not None):\n",
    "        df = df[df.Category == cat]\n",
    "    f, axtuple = plt.subplots(1, 2, sharey=True)\n",
    "    colors = {\"Weekend\": plt.cm.Greens, \"Weekday\": plt.cm.Purples}\n",
    "    for i, day in enumerate([\"Weekday\", \"Weekend\"]):\n",
    "        subset = df[df.Weekend == day]\n",
    "        plt.sca(axtuple[i])\n",
    "        hexbin_image(subset, title='%s during the %s' % (df.Category.mode().values[0], day), color=colors[day])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day_vs_night_plot(train_df, cat=\"LARCENY/THEFT\")\n",
    "plt.gcf().set_size_inches(16, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day_vs_night_plot(train_df, cat=\"ASSAULT\")\n",
    "plt.gcf().set_size_inches(16, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weekday_weekend_plot(train_df, cat=\"ASSAULT\")\n",
    "plt.gcf().set_size_inches(16, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def heat_plot(df, category):\n",
    "    y, x = np.mgrid[0:25, 0:8]\n",
    "    z = np.zeros(y.shape)\n",
    "    grouped = train_df.groupby(\"Category\")\n",
    "    subset = grouped.get_group(category)\n",
    "    more_groups = subset.groupby([\"DOW_Index\", \"Hour\"])\n",
    "    for name, group in more_groups:\n",
    "        z[name[1], name[0]] = len(group)\n",
    "    plt.pcolor(x, y, z, cmap='YlOrRd')\n",
    "    plt.gca().set_xticklabels(dow)\n",
    "    plt.ylabel(\"Hour\")\n",
    "    plt.title(\"Day-time distribution of %s\" % category)\n",
    "    plt.axis([x.min(), x.max(), y.min(), y.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "padded = all_categories.tolist()\n",
    "while (len(padded) < 13*3):\n",
    "    padded.append(None)\n",
    "cat_array = np.reshape(padded, (13, 3))\n",
    "f, axtuple = plt.subplots(cat_array.shape[0], cat_array.shape[1], sharey=True)\n",
    "rows, cols = cat_array.shape\n",
    "\n",
    "for j in range(rows):\n",
    "    for i in range(cols):\n",
    "        cat = cat_array[j, i]\n",
    "        if (cat is not None):\n",
    "            plt.sca(axtuple[j][i])\n",
    "            heat_plot(train_df, cat)\n",
    "            plt.xticks(rotation=25)\n",
    "plt.gcf().set_size_inches(16, 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For certain crimes it seems like noon and midnight are special, presumably because that's when the crimes are recorded. The crimes where this is seen are\n",
    "- Fraud\n",
    "- Forgery/Counterfeiting\n",
    "- Sex Offenses Forcible\n",
    "- Embezzlement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are some helper functions we'll use later to give our models the parameters we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_x_matrix(df, formula):\n",
    "    x_vals = dmatrix(formula, data=df, return_type='dataframe')\n",
    "    return x_vals\n",
    "        \n",
    "def get_x_y_matrices(df, formula, target, filt=True):\n",
    "    \n",
    "    x_vals = dmatrix(formula, data=df, return_type='dataframe')\n",
    "    y_vals = df[target]\n",
    "    if (filt):\n",
    "        return filter_infrequent(x_vals, y_vals)\n",
    "    return x_vals, y_vals\n",
    "    \n",
    "def filter_infrequent(x, y, threshold=3):\n",
    "    counts = y.value_counts()\n",
    "    for cat, count in counts.iteritems():\n",
    "        if count < 3:\n",
    "            x = x[y != cat]\n",
    "            y = y[y != cat]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are some of the algorithms we used. We subclass them so that the `cross_val_predict` function uses probabilities, not outright predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ProbaRandomForestClassifier(RandomForestClassifier):\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X)\n",
    "        \n",
    "class ProbaLogisticRegression(LogisticRegression):\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X)\n",
    "                                  \n",
    "        \n",
    "class ProbaBernoulliNB(BernoulliNB):\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a Weighted Averager Class for when we want to compare how best to weight an ensemble algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Weighted_Averager(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, models, weights):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        self.probas = [len(models)]\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        for m in self.models:\n",
    "            m.fit(x, y)\n",
    "            \n",
    "    def predict_probas_cv(self, x, y):\n",
    "        probas = [None] * len(self.models)\n",
    "        for i, m in enumerate(self.models):\n",
    "            x_curr, y_curr = x, y\n",
    "            if type(x) is list:\n",
    "                x_curr = x[i]\n",
    "            if type(y) is list:\n",
    "                y_curr = y[i]\n",
    "            probas[i] = cross_val_predict(m, x_curr, y_curr)\n",
    "        self.probas = probas\n",
    "            \n",
    "    def calc_log_loss(self, y, weights=None):\n",
    "        if (weights is None):\n",
    "            weights = self.weights\n",
    "        res = None\n",
    "        for p, w in zip(self.probas, weights):\n",
    "            if res is None:\n",
    "                res = w * p\n",
    "            else:\n",
    "                res += w * p\n",
    "        return log_loss(y, res)\n",
    "            \n",
    "    def predict_proba(self, x):\n",
    "        res = None\n",
    "        for m, w in zip(self.models, self.weights):\n",
    "            if res is None:\n",
    "                res = w * m.predict_proba(x)\n",
    "            else:\n",
    "                res += w * m.predict_proba(x)\n",
    "                \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a Random Forest with ALL the features we had initially!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formula_ml = 'C(DayOfWeek) + C(PdDistrict) + C(Street_Corner) + X+Y+Hour+Month + Hour_Minutes + Weekend + WorkNextDay + Minutes_Since_New_Year + DayorNight + Hour:DayOfWeek'\n",
    "x_vals, y_vals = get_x_y_matrices(train_df, formula_ml, 'Category')\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(min_samples_leaf = 300)\n",
    "\n",
    "print cross_val_score(clf, x_vals, y_vals, scoring='log_loss').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(min_samples_leaf=300)\n",
    "\n",
    "forest.fit(x_vals, y_vals)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "for i, c in zip(importances, x_vals.columns):\n",
    "    print \"%.3f\" % i + \": \" + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a  RandomForest with the just the features we're sure are important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formula_ml = 'C(PdDistrict) + C(Street_Corner) + X+Y+Hour'\n",
    "\n",
    "x_vals, y_vals = get_x_y_matrices(train_df, formula_ml, 'Category')\n",
    "min_leaves = np.round_(np.logspace(1, 3.5, num=10)) # values from 10 to 3100\n",
    "n_estimators = [10, 30, 100]\n",
    "\n",
    "parameters = {'min_samples_leaf':min_leaves, 'n_estimators': n_estimators}\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(), parameters, scoring='log_loss')\n",
    "\n",
    "clf.fit(x_vals, y_vals)\n",
    "\n",
    "scores = [[], [], []]\n",
    "\n",
    "for i, f in enumerate(clf.grid_scores_):\n",
    "    scores[i % 3].append(f[1])\n",
    "    \n",
    "for n, score in zip(n_estimators, scores):\n",
    "    \n",
    "    plt.semilogx(min_leaves, score, label=\"%i estimators\" % n)\n",
    "    plt.xlabel('min_sample_leaf parameter')\n",
    "    plt.ylabel('log-loss score')\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We found that a min_samples_leaf parameter of about 300 is optimal for our Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formula_ml = 'C(PdDistrict) + C(Street_Corner) + X+Y+Hour '\n",
    "x_vals, y_vals = get_x_y_matrices(train_df, formula_ml, 'Category')\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(min_samples_leaf = 300)\n",
    "\n",
    "print cross_val_score(clf, x_vals, y_vals, scoring='log_loss').mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how well our model predicts each category of crime\n",
    "##### The graph below shows our log-loss score for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formula_ml = 'C(PdDistrict) + Street_Corner + X+Y+Hour'\n",
    "\n",
    "x_vals, y_vals = get_x_y_matrices(train_df, formula_ml, 'Category')\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_vals)\n",
    "y_vals_matrix = lb.transform(y_vals)\n",
    "\n",
    "clf_forest = ProbaRandomForestClassifier(min_samples_leaf=300)\n",
    "\n",
    "predictions = cross_val_predict(clf_forest, x_vals, y_vals)\n",
    "crimes = lb.classes_\n",
    "scores = []\n",
    "\n",
    "for i, crime in enumerate(crimes):\n",
    "    indexor = (y_vals == crime).values\n",
    "    scores.append(log_loss(y_vals_matrix[indexor], predictions[indexor]))\n",
    "    \n",
    "ax = sns.barplot(x=crimes, y=scores)\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=90)\n",
    "    \n",
    "plt.gcf().set_size_inches(16, 6, forward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While the plot above is interesting, we will get more useful data if we normalize it for frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = y_vals.value_counts()\n",
    "#print counts\n",
    "weighted_scores = []\n",
    "for i, crime in enumerate(crimes):\n",
    "    weighted_scores.append(scores[i] * counts[crime])\n",
    "ax = sns.barplot(x=crimes, y=weighted_scores)\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=90)\n",
    "    \n",
    "plt.gcf().set_size_inches(16, 6, forward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The categories in which our model suffers from the highest log-loss values are the categories of crimes that are the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ', '.join(all_categories[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try adding the midnight and noon features to our random forest.\n",
    "##### This decision was made based on what we saw in the heat map plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formula_ml = 'C(PdDistrict) + C(Street_Corner) + X+Y+Hour + Midnight + Noon'\n",
    "x_vals, y_vals = get_x_y_matrices(train_df, formula_ml, 'Category')\n",
    "clf = RandomForestClassifier(min_samples_leaf = 300)\n",
    "print cross_val_score(clf, x_vals, y_vals, scoring='log_loss').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We worked with some other algorithms as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naieve Bayes BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = BernoulliNB()\n",
    "print cross_val_score(clf, x_vals, y_vals, scoring='log_loss').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the Naieve Bayes Bernoulli algorithm are better than our original Random Forest with no parameter tweaking, but not as good as the Random Forest after overfitting has been accounted for by tuning its parameters.\n",
    "Presumably, this algorithm does so well with log-loss scoring because it is never very certain of its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formula_ml = 'C(PdDistrict) + C(Street_Corner) + DayorNight + DayOfWeek + Midnight + Noon'\n",
    "x_vals, y_vals = get_x_y_matrices(train_df, formula_ml, 'Category')\n",
    "clf = LogisticRegression()\n",
    "print cross_val_score(clf, x_vals, y_vals, scoring='log_loss').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, our random forest has performed better than both the Bernulli and Logistic Regression algorithms,but we wanted to see if by ensembling/weighting multiple algorithms together we could achieve something greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_forest = ProbaRandomForestClassifier(min_samples_leaf=300)\n",
    "clf_logistic = ProbaLogisticRegression()\n",
    "clf_bernoulli = ProbaBernoulliNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing best proportiton of Random Forest to Bernoulli\n",
    "We wanted to see whether a combination of Random Forest and Bernoulli would work better than the random forest alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formula_ml = 'C(PdDistrict) + C(Street_Corner) + X+Y+Hour + Midnight + Noon'\n",
    "x_vals, y_vals = get_x_y_matrices(train_df, formula_ml, 'Category')\n",
    "\n",
    "w_a = Weighted_Averager([clf_forest, clf_bernoulli], [0.5, 0.5])\n",
    "w_a.predict_probas_cv(x_vals, y_vals)\n",
    "weights = np.linspace(0.1, 0.9, 9)\n",
    "scores = []\n",
    "for w in weights:\n",
    "    s = w_a.calc_log_loss(y_vals, weights=[w, 1-w])\n",
    "    scores.append(s)\n",
    "    \n",
    "plt.plot(weights, scores)\n",
    "plt.xlabel(\"Percentage forest\")\n",
    "plt.title(\"An ensemble of a forest and a bernoulli model\")\n",
    "plt.ylabel(\"score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It appears that the more heavily we weight the forest and the less heavily we weight Bernoulli, the better our model scores. Because of this, we have chosen to not use Bernoulli in our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formula_ml = 'C(PdDistrict) + C(Street_Corner) + X+Y+Hour + Midnight + Noon'\n",
    "x_vals, y_vals = get_x_y_matrices(train_df, formula_ml, 'Category')\n",
    "formula_ml_log = 'C(PdDistrict) + C(Street_Corner) + DayorNight + DayOfWeek + Midnight + Noon'\n",
    "x_vals_log, y_vals_log = get_x_y_matrices(train_df, formula_ml_log, 'Category')\n",
    "w_a = Weighted_Averager([clf_forest, clf_logistic], [0.5, 0.5])\n",
    "w_a.predict_probas_cv([x_vals, x_vals_log], y_vals)\n",
    "weights = np.linspace(0.1, 0.9, 9)\n",
    "scores = []\n",
    "for w in weights:\n",
    "    s = w_a.calc_log_loss(y_vals, weights=[w, 1-w])\n",
    "    scores.append(s)    \n",
    "plt.plot(weights, scores)\n",
    "plt.title(\"An ensemble of a forest and a logistic model\")\n",
    "plt.xlabel(\"Percentage forest\")\n",
    "plt.ylabel(\"score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It appears once again that the more heavily we weight the forest, the better our model scores. Because of this, we have chosen to  use the Random Forest in our final model without ensembling with other algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to .csv for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alg = RandomForestClassifier(min_samples_leaf= 300)\n",
    "formula_ml = 'C(PdDistrict) + C(Street_Corner) + X+Y+Hour + Midnight + Noon'\n",
    "x_train, y_train = get_x_y_matrices(train_df, formula_ml, 'Category')\n",
    "alg.fit(x_train, y_train)\n",
    "predictions = alg.predict_proba(get_x_matrix(preprocess(test,drop=False), formula_ml))\n",
    "predictions = pd.DataFrame(predictions, index=test.index, columns=alg.classes_)\n",
    "predictions.to_csv(\"predictions.csv\", index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Our Model Scored: 2.42\n",
    "### 215 on the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
